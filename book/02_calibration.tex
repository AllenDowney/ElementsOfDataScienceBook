\hypertarget{calibration}{%
\chapter{Calibration}\label{calibration}}

In the previous chapter, we replicated the analysis reported in
``Machine Bias'', by Julia Angwin, Jeff Larson, Surya Mattu and Lauren
Kirchner, published by ProPublica in May 2016.

After the ProPublica article, the Washington Post published a response
by Sam Corbett-Davies, Emma Pierson, Avi Feller and Sharad Goel: ``A
computer program used for bail and sentencing decisions was labeled
biased against blacks. It's actually not that clear.''.

I encourage you to read both of those articles before you go on. In this
chapter, I explain some of the arguments presented in the Washington
Post (WaPo) article, and we will replicate their analysis.

\hypertarget{the-wapo-response}{%
\section{The WaPo Response}\label{the-wapo-response}}

The Washington Post article summarizes the ProPublica article and the
response from Northpointe, the company that makes COMPAS, like this:

\begin{itemize}
\item
  ProPublica claims that COMPAS is unfair because ``among defendants who
  ultimately did not reoffend, blacks were more than twice as likely as
  whites to be classified as medium or high risk.''
\item
  Northpointe claims that COMPAS is fair because ``scores mean
  essentially the same thing regardless of the defendant's race. For
  example, among defendants who scored a seven on the COMPAS scale, 60
  percent of white defendants reoffended, which is nearly identical to
  the 61 percent of black defendants who reoffended.''
\end{itemize}

So ProPublica and Northpointe are invoking different definitions of
``fair''.

In the previous chapter we explored the first definition by computing
error rates (false positive and false negative) for white and black
defendants. In this chapter, we'll explore the second definition, which
is called ``calibration''.

\hypertarget{calibration-1}{%
\section{Calibration}\label{calibration-1}}

The WaPo article includes this figure, which shows ``white and black
defendants with the same risk score are roughly equally likely to
reoffend.''

\includegraphics{figs/calibration1.png}

To understand this figure, let's start by replicating it.

The following function groups defendants by risk score and computes the
fraction in each group that were charged with another crime within two
years.

\begin{lstlisting}[language=Python,style=source]
def calibration_curve(df):
    """Fraction in each risk group charged with another crime.
    
    df: DataFrame
    
    returns: Series
    """
    grouped = df.groupby('decile_score')
    return grouped['two_year_recid'].mean()
\end{lstlisting}

The following figure shows this calibration curve for all defendants and
for white and black defendants separately.

\begin{lstlisting}[language=Python,style=source]
cal_all = calibration_curve(cp)
cal_all.plot(linestyle='dotted', 
              color='gray',
              label='All defendants')

white = (cp['race'] == 'Caucasian')
cal_white = calibration_curve(cp[white])
cal_white.plot(label='White')

black = (cp['race'] == 'African-American')
cal_black = calibration_curve(cp[black])
cal_black.plot(label='Black')

decorate(xlabel='Risk score',
         ylabel='Fraction charged with new crime',
         title='Recivism vs risk score, grouped by race')
\end{lstlisting}

\begin{center}
\includegraphics[scale=0.75]{02_calibration_files/02_calibration_21_0.pdf}
\end{center}

This figure shows that people with higher risk scores are more likely to
be charged with a new crime within two years. In that sense COMPAS works
as intended.

Furthermore, the test is equally calibrated for black and white
defendants; that is, in each risk group, the rate of recidivism is about
the same for black and white defendants.

The WaPo article explains why this is important:

\begin{quote}
A risk score of seven for black defendants should mean the same thing as
a score of seven for white defendants. Imagine if that were not so, and
we systematically assigned whites higher risk scores than equally risky
black defendants with the goal of mitigating ProPublica's criticism. We
would consider that a violation of the fundamental tenet of equal
treatment.
\end{quote}

So we want a test that has the same calibration for all groups, and we
want a test that has the same error rates for all groups. But there's
the problem: as the WaPo article explains, it is mathematically
impossible to be fair by both definitions at the same time.

To see why, let's go back to the confusion matrix.

\hypertarget{matrices-and-metrics}{%
\section{Matrices and Metrics}\label{matrices-and-metrics}}

In the previous chapter, we computed confusion matrices for white and
black defendants. Here they are again:

\begin{lstlisting}[language=Python,style=source]
matrix_white = make_matrix(cp[white])
matrix_white
\end{lstlisting}

\begin{tabular}{lrr}
\toprule
Actual &  Condition &  No Condition \\
Predicted &            &               \\
\midrule
Positive  &        505 &           349 \\
Negative  &        461 &          1139 \\
\bottomrule
\end{tabular}

\begin{lstlisting}[language=Python,style=source]
matrix_black = make_matrix(cp[black])
matrix_black
\end{lstlisting}

\begin{tabular}{lrr}
\toprule
Actual &  Condition &  No Condition \\
Predicted &            &               \\
\midrule
Positive  &       1369 &           805 \\
Negative  &        532 &           990 \\
\bottomrule
\end{tabular}

And here are the metrics we computed from the confusion matrices:

\begin{lstlisting}[language=Python,style=source]
metrics_white = compute_metrics(matrix_white, 
                                'White defendants')
metrics_white
\end{lstlisting}

\begin{tabular}{lr}
\toprule
{} &    Percent \\
White defendants &            \\
\midrule
FPR              &  23.454301 \\
FNR              &  47.722567 \\
PPV              &  59.133489 \\
NPV              &  71.187500 \\
Prevalence       &  39.364303 \\
\bottomrule
\end{tabular}

\begin{lstlisting}[language=Python,style=source]
metrics_black = compute_metrics(matrix_black, 
                                'Black defendants')
metrics_black
\end{lstlisting}

\begin{tabular}{lr}
\toprule
{} &    Percent \\
Black defendants &            \\
\midrule
FPR              &  44.846797 \\
FNR              &  27.985271 \\
PPV              &  62.971481 \\
NPV              &  65.045992 \\
Prevalence       &  51.433983 \\
\bottomrule
\end{tabular}

If we look at the error rates (FPR and FNR), it seems like COMPAS is
biased against black defendants:

\begin{itemize}
\item
  Their false positive rate is higher (45\% vs 23\%): among people who
  \emph{will not} recidivate, black defendants are more likely to be
  classified high risk.
\item
  Their false negative rate is lower (28\% vs 48\%): among people who
  \emph{will} recidivate, black defendants are less likely to be
  classified low risk.
\end{itemize}

But if we look at the the predictive values (PPV and NPV) it seems like
COMPAS is biased in favor of black defendants:

\begin{itemize}
\item
  Among people in the \emph{high risk group}, black defendants are more
  likely to be charged with another crime (63\% vs 59\%).
\item
  Among people in the \emph{low risk group}, black defendants are less
  likely to ``survive'' two years without another charge (65\% vs 71\%).
\end{itemize}

It seems like we should be able to fix these problems, but it turns out
that we can't. We'll see why in the next section.

\hypertarget{what-would-it-take}{%
\section{What Would It Take?}\label{what-would-it-take}}

Suppose we want to fix COMPAS so that predictive values are the same for
black and white defendants. We could do that by using different
thresholds for the two groups.

In this section, we'll figure out what it would take to re-calibrate
COMPAS; then we'll see what effect that would have on predictive values.

The following function loops through possible thresholds, makes the
confusion matrix with each threshold, and computes accuracy metrics.

\begin{lstlisting}[language=Python,style=source]
def sweep_threshold(cp):
    """Sweep a range of threshold and compute accuracy metrics.
    
    cp: DataFrame of COMPAS data
    
    returns: DataFrame with one row for each threshold and
             one column for each metric
    """
    index = range(0, 11)
    columns = ['FPR', 'FNR', 'PPV', 'NPV', 'Prevalence']
    table = pd.DataFrame(index=index, columns=columns, dtype=float) 

    for threshold in index:
        m = make_matrix(cp, threshold)
        metrics = compute_metrics(m)
        table.loc[threshold] = metrics['Percent']
        
    return table
\end{lstlisting}

Here's the resulting table for all defendants.

\begin{lstlisting}[language=Python,style=source]
table_all = sweep_threshold(cp)
table_all.head()
\end{lstlisting}

\begin{tabular}{lrrrrr}
\toprule
{} &         FPR &        FNR &        PPV &        NPV &  Prevalence \\
\midrule
0 &  100.000000 &   0.000000 &  45.065151 &        NaN &   45.065151 \\
1 &   71.435781 &   9.474008 &  50.969865 &  78.611111 &   45.065151 \\
2 &   55.084532 &  18.486620 &  54.831368 &  74.758505 &   45.065151 \\
3 &   43.325763 &  27.130114 &  57.978463 &  71.803069 &   45.065151 \\
4 &   32.349230 &  37.403876 &  61.350618 &  68.796510 &   45.065151 \\
\bottomrule
\end{tabular}

The following figure shows error rates as a function of threshold.

\begin{lstlisting}[language=Python,style=source]
table_all['FPR'].plot(color='C2')
table_all['FNR'].plot(color='C4')

decorate(xlabel='Threshold', 
         ylabel='Percent',
         title='Error rates for a range of thresholds')
\end{lstlisting}

\begin{center}
\includegraphics[scale=0.75]{02_calibration_files/02_calibration_37_0.pdf}
\end{center}

When the threshold is low, almost everyone is in the high risk group; in
that case:

\begin{itemize}
\item
  FNR is low because most recidivists are in the high risk group, but
\item
  FPR is high because most non-recidivists are \emph{also} in the high
  risk group.
\end{itemize}

When the threshold is high, almost everyone is in the low risk group,
and the metrics are the other way around:

\begin{itemize}
\item
  FPR is low because most non-recidivists are in the low risk group, but
\item
  FNR is high because most recidivists are \emph{also} in the low risk
  group.
\end{itemize}

The following figure shows predictive values for a range of thresholds.

\begin{lstlisting}[language=Python,style=source]
table_all['PPV'].plot(color='C0')
table_all['NPV'].plot(color='C1')

decorate(xlabel='Threshold', 
         ylabel='Percent',
         title='Predictive values for a range of thresholds')
\end{lstlisting}

\begin{center}
\includegraphics[scale=0.75]{02_calibration_files/02_calibration_39_0.pdf}
\end{center}

When the threshold is too low, PPV is low. When the threshold is too
high, NPV is low.

Now I'll compute tables for black and white defendants separately.

\begin{lstlisting}[language=Python,style=source]
table_white = sweep_threshold(cp[white])
table_black = sweep_threshold(cp[black])
\end{lstlisting}

I'll use the following function to interpolate columns in the table;
that is, for a given threshold I can compute the corresponding metric.

\begin{lstlisting}[language=Python,style=source]
from scipy.interpolate import interp1d

def interpolate(series, value, **options):
    """Evaluate a function at a value.
    
    series: Series
    value: number
    options: passed to interp1d (default is linear interp)
    
    returns: number
    """
    interp = interp1d(series.index, series.values, **options)
    return interp(value)
\end{lstlisting}

The following function goes the other way: it estimates the threshold
where a column passes through a given metric.

\begin{lstlisting}[language=Python,style=source]
def crossing(series, value, **options):
    """Find where a function crosses a value.
    
    series: Series
    value: number
    options: passed to interp1d (default is linear interp)
    
    returns: number
    """
    interp = interp1d(series.values, series.index, **options)
    return interp(value)
\end{lstlisting}

We can use \passthrough{\lstinline!crossing!} to calibrate the test for
white defendants; that is, we can compute the threshold that would make
the error rates for white defendants the same as for the general
population.

\begin{lstlisting}[language=Python,style=source]
matrix_all = make_matrix(cp)
fpr, fnr = error_rates(matrix_all)
\end{lstlisting}

\begin{lstlisting}[language=Python,style=source]
crossing(table_white['FPR'], fpr)
\end{lstlisting}

\begin{lstlisting}[style=output]
array(3.23048519)
\end{lstlisting}

\begin{lstlisting}[language=Python,style=source]
crossing(table_white['FNR'], fnr)
\end{lstlisting}

\begin{lstlisting}[style=output]
array(3.11788885)
\end{lstlisting}

With a threshold near 3.2, white defendants would have the same error
rates as the general population. Now let's do the same computation for
black defendants.

\begin{lstlisting}[language=Python,style=source]
crossing(table_black['FPR'], fpr)
\end{lstlisting}

\begin{lstlisting}[style=output]
array(5.20906103)
\end{lstlisting}

\begin{lstlisting}[language=Python,style=source]
crossing(table_black['FNR'], fnr)
\end{lstlisting}

\begin{lstlisting}[style=output]
array(5.01417524)
\end{lstlisting}

To get the same error rates for black and white defendants, we would
need different thresholds: about 5.1 compared to 3.2. At those levels,
the predictive values are substantially different. Here's PPV for each
group with different thresholds:

\begin{lstlisting}[language=Python,style=source]
interpolate(table_white['PPV'], 3.2)
\end{lstlisting}

\begin{lstlisting}[style=output]
array(55.23319482)
\end{lstlisting}

\begin{lstlisting}[language=Python,style=source]
interpolate(table_black['PPV'], 5.1)
\end{lstlisting}

\begin{lstlisting}[style=output]
array(66.21639173)
\end{lstlisting}

With equal error rates, we get different PPV:

\begin{itemize}
\item
  Among white defendants in the high risk group, about 55\% would
  recidivate.
\item
  Among black defendants in the high risk group, about 66\% would
  recidivate.
\end{itemize}

Here's NPV for each group with different thresholds:

\begin{lstlisting}[language=Python,style=source]
interpolate(table_white['NPV'], 3.2)
\end{lstlisting}

\begin{lstlisting}[style=output]
array(73.06639734)
\end{lstlisting}

\begin{lstlisting}[language=Python,style=source]
interpolate(table_black['NPV'], 5.1)
\end{lstlisting}

\begin{lstlisting}[style=output]
array(62.16782561)
\end{lstlisting}

With equal error rates, the NPVs are substantially different:

\begin{itemize}
\item
  Among white defendants in the low risk group, 73\% survive two years
  without another charge.
\item
  Among black defendants in the low risk group, 62\% survive.
\end{itemize}

To summarize, if the test is calibrated in terms of error rates, it is
not calibrated in terms of predictive values.

\begin{itemize}
\item
  If we make the error rates more equal, we make the predictive values
  more unfair, and
\item
  If we make the predictive values more equal, we make the error rates
  more unfair.
\end{itemize}

Fundamentally, the problem is that the prevalence of recidivism is
different in the two groups: about 39\% of white defendants were charged
with another crime within two years, compared to 51\% of black
defendants.

As long as that's the case (for any two groups) the predictive values
and error rates can't be ``fair'' at the same time.

\hypertarget{roc}{%
\section{ROC}\label{roc}}

In the previous section I plotted various metrics as as function of
threshold. Another common and useful way to visualize these results is
to plot sensitivity (which is the complement of FNR) versus FPR. For
historical reasons, the result is called a receiver operating
characteristic (ROC) curve (see
\url{https://en.wikipedia.org/wiki/Receiver_operating_characteristic}).

The following function plots the ROC curve:

\begin{lstlisting}[language=Python,style=source]
def plot_roc(table, **options):
    """Plot the ROC curve.
    
    table: DataFrame of metrics as a function of 
           classification threshold
    options: passed to plot
    """
    plt.plot([0,100], [0,100], ':', color='gray')
    sens = 100-table['FNR']
    plt.plot(table['FPR'], sens, **options)
    decorate(xlabel='FPR',
             ylabel='Sensitivity (1-FNR)',
             title='ROC curve')
\end{lstlisting}

Here's the ROC curve for all defendants.

\begin{lstlisting}[language=Python,style=source]
plot_roc(table_all, color='C2', label='All defendants')
\end{lstlisting}

\begin{center}
\includegraphics[scale=0.75]{02_calibration_files/02_calibration_64_0.pdf}
\end{center}

The green line is the ROC curve. The gray dotted line shows the identity
line for comparison.

An ideal test would have high sensitivity for all values of FPR, but in
reality there is almost always a trade-off:

\begin{itemize}
\item
  When FPR is low, sensitivity is low.
\item
  In order to get more sensitivity, we have to accept a higher FPR.
\end{itemize}

The ROC curve tells us how much sensitivity we get for a given FPR or,
the other way around, how much FPR we have to accept to achieve a given
sensitivity. The following figure shows the ROC curves for white and
black defendants.

\begin{lstlisting}[language=Python,style=source]
plot_roc(table_white)
plot_roc(table_black)
\end{lstlisting}

\begin{center}
\includegraphics[scale=0.75]{02_calibration_files/02_calibration_66_0.pdf}
\end{center}

The ROC curves are similar for the two groups, which shows that we can
achieve nearly the same error rates (FPR and FNR) for the two groups, as
we did in the previous section. It also shows that the test has nearly
the same ``concordance'' for both groups, which I explain in the next
section.

\hypertarget{auc}{%
\section{AUC}\label{auc}}

The authors of the ProPublica article published a supplementary article,
``How We Analyzed the COMPAS Recidivism Algorithm'', which describes
their analysis in more detail (see
\url{https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm}).

As another metric of accuracy, they estimate \textbf{concordance}, which
they describe like this:

\begin{quote}
Overall, {[}COMPAS has{]} a concordance score of 63.6 percent. That
means for any randomly selected pair of defendants in the sample, the
COMPAS system can accurately rank their recidivism risk 63.6 percent of
the time (e.g.~if one person of the pair recidivates, that pair will
count as a successful match if that person also had a higher score). In
its study, Northpointe reported a slightly higher concordance: 68
percent.
\end{quote}

They explain:

\begin{quote}
{[}These estimates{]} are lower than what Northpointe describes as a
threshold for reliability. ``A rule of thumb according to several recent
articles is that AUCs of .70 or above typically indicate satisfactory
predictive accuracy, and measures between .60 and .70 suggest low to
moderate predictive accuracy,'' the company says in its study.
\end{quote}

There are several ways to compute concordance, but one of the simplest
is to compute the area under the ROC curve. If you would like to know
why this works, you might find this discussion helpful:
\url{https://stats.stackexchange.com/questions/180638/how-to-derive-the-probabilistic-interpretation-of-the-auc}.

Since we've already computed the ROC, we can use Simpson's rule to
estimate the area under the curve (see
\url{https://en.wikipedia.org/wiki/Simpsons_rule}).

\begin{lstlisting}[language=Python,style=source]
from scipy.integrate import simps

def compute_auc(table):
    """Compute the area under the ROC curve.
    
    Uses the trapezoid rule, so 
    """
    y = 100-table['FNR']
    x = table['FPR']
    y = y.sort_index(ascending=False) / 100
    x = x.sort_index(ascending=False) / 100
    return simps(y.values, x.values)
\end{lstlisting}

The concordance (AUC) for all respondents is about 70\%.

\begin{lstlisting}[language=Python,style=source]
compute_auc(table_all)
\end{lstlisting}

\begin{lstlisting}[style=output]
0.7059974460900778
\end{lstlisting}

For the subgroups it is slightly lower, but also near 70\%.

\begin{lstlisting}[language=Python,style=source]
compute_auc(table_white)
\end{lstlisting}

\begin{lstlisting}[style=output]
0.6995093159638619
\end{lstlisting}

\begin{lstlisting}[language=Python,style=source]
compute_auc(table_black)
\end{lstlisting}

\begin{lstlisting}[style=output]
0.694567757058852
\end{lstlisting}

Different ways of computing concordance handle ties differently, which
is probably why we, ProPublica, and Northpointe get somewhat different
estimates. But qualitatively they all tell the same story; as a binary
classifier, COMPAS is only moderately accurate. However, it seems to be
equally accurate, by this metric, for white and black defendants.

\hypertarget{summary}{%
\section{Summary}\label{summary}}

In this chapter, we replicated the analysis reported in the WaPo article
and confirmed two of the arguments they presented:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  COMPAS is calibrated in the sense that white and black defendants with
  the same risk score have almost the same probability of being charged
  with another crime. This implies that it has roughly the same
  predictive value for both groups.
\item
  It is impossible for COMPAS to have the same predictive values for
  both groups and the same error rates at the same time.
\end{enumerate}

And we showed:

\begin{itemize}
\item
  If you design a test to achieve equal predictive value across groups
  with different prevalence, you will find that error rates differ.
  Specifically, false positive rates will be higher in groups with
  higher recividism.
\item
  If you design a test to achieve equal error rates across groups, you
  will find that predictive values differ. Specifically, positive
  predictive value will be lower in groups with lower rates of
  recidivism.
\end{itemize}

Finally, we derived the ROC curve and computed AUC, which shows that
COMPAS has nearly the same concordance for white and black defendants.

