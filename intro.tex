\section{Introduction}\label{introduction}}

{\it Elements of Data Science} is an introduction to data science for
people with no programming experience. My goal is to present a small,
powerful subset of Python that allows you to do real work in data
science as quickly as possible.

I won't assume you know anything about programming,
statistics, or data science. When I use a term, I'll define it
immediately, and when I use a programming feature, I'll explain it.

For each chapter in this book, there is a Jupyter notebook you can access from \url{https://allendowney.github.io/ElementsOfDataScience}. Jupyter is a software
development tool you can run in a web browser, so you don't have to
install any software. A Jupyter notebook is a document that contains
text, Python code, and results. So you can read it like a book, but you
can also modify the code, run it, develop new programs, and test them.

The notebooks contain exercises where you can practice what you learn.
Most of the exercises are meant to be quick, but a few are more
substantial.

\section{Book Overview}

This book is organized in five parts.

Part I is an introduction to Python with emphasis on concepts and tools for working with data. It introduces Python data structures like lists and dictionaries, NumPy arrays, and Pandas DataFrames.

Part II is about exploratory data analysis, starting with the ways we represent and summarize the distribution of a set of values, and ending with linear and logistic regression.

Part III is about statistical inference, that is, using data to infer the properties of a population. It introduces randomization methods, especially bootstrap resampling, as a tool for estimating a quantity, describing the precision of the estimate, and testing hypotheses.

Part IV is the first of two case studies, an exploration of data from the General Social Survey. In introduces tools for describing changes over time and differences between groups, including cross tabulations and pivot tables.

Part V is the second case study, which introduces classification algorithms and the metrics we use to evaluate their performance, applied to a particularly challenging topic in the criminal justice system, the use of algorithms to predict who is most likely to commit future crimes.

Here are more detailed descriptions of the chapters:

\begin{description}

\item Chapter 1 explains how to use
Jupyter and introduces variables, values, and numerical computation.

\item Chapter 2 shows how to represent times,
dates, and locations in Python, and uses the GeoPandas library to plot
points on a map.

\item Chapter 3 presents lists and NumPy
arrays. It discusses absolute, relative, and percent errors, and ways to
summarize them.

\item Chapter 4 presents the \texttt{for} loop
and the \texttt{if} statement; then it uses them to speed-read \emph{War
and Peace} and count the words.

\item Chapter 5 presents one of the most powerful
features of Python, dictionaries, and uses them to count the unique
words in a text and their frequencies.

\item Chapter 6 introduces a plotting library,
Matplotlib, and uses it to generate a few common data visualizations and
one less common one, a Zipf plot.

\item Chapter 7 presents DataFrames, which are used
to represent tables of data. As an example, it uses data from the
National Survey of Family Growth to find the average weight of babies in
the U.S.

\item Chapter 8 explains what a distribution is
and presents 3 ways to represent one: a PMF, CDF, or PDF. It also shows
how to compare a distribution to another distribution or a mathematical
model.

\item Chapter 9 explores relationships between
variables using scatter plots, violin plots, and box plots. It
quantifies the strength of a relationship using the correlation
coefficient and uses simple regression to estimate the slope of a line.

\item Chapter 10 presents multiple regression and uses
it to explore the relationship between age, education, and income. It
uses visualization to interpret multivariate models. It also presents
binary variables and logistic regression.

\item Chapter 11 presents computational methods we can
use to quantify variation due to random sampling, which is one of
several sources of error in statistical estimation.

\item Chapter 12 introduces bootstrapping, a kind of resampling that is
well suited to the kind of survey data we've been working with.

\item Chapter 13 uses a computational approach to explain hypothesis testing, which is the bugbear of classical statistics.

\item Chapter 14 is the beginning of a case study that uses data from the General Social Survey. It uses survey responses to explore political alignment and polarization in the United States over the last 50 years.

\item Chapter 15 is the second part of the case study; it explores the relationship between political alignment (conservative, moderate, or liberal) and other attitudes and beliefs.

\item Chapter 16 introduces the second case study, related to an algorithm used in the criminal justice system to predict crime. It replicates analysis reported in 2016 to evaluate the performance of the algorithm and its fairness between racial groups.

\item Chapter 17 extends the analysis in the previous chapter to address fairness between men and women, and explores difference definitions of fairness and why they are hard to achieve.
